import torch
import torch.nn.functional as F
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from torch_geometric.utils import from_networkx
from torch_geometric.nn import GATConv, global_mean_pool
from rdkit import Chem
import networkx as nx
import shap

# Define atom features
def atom_features(atom):
    return [
        atom.GetAtomicNum(),
        atom.GetDegree(),
        atom.GetHybridization(),
        atom.GetIsAromatic(),
        atom.GetTotalNumHs(),
        atom.GetExplicitValence(),
        atom.GetNumImplicitHs(),
        atom.GetFormalCharge()
    ]

# Convert SMILES to graph structure
def smiles_to_graph(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        raise ValueError(f"Invalid SMILES: {smiles}")

    G = nx.Graph()
    for atom in mol.GetAtoms():
        G.add_node(atom.GetIdx(), x=torch.tensor(atom_features(atom), dtype=torch.float))
    for bond in mol.GetBonds():
        G.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())

    data = from_networkx(G)
    return data

# Define GAT layer
class GATLayer(nn.Module):
    def __init__(self, in_channels, out_channels, heads=8):
        super(GATLayer, self).__init__()
        self.conv = GATConv(in_channels, out_channels, heads=heads)

    def forward(self, x, edge_index):
        return self.conv(x, edge_index)

# Define MPNN model using GAT
class MPNNModel(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, heads=4):
        super(MPNNModel, self).__init__()
        self.layers = torch.nn.ModuleList()
        self.layers.append(GATLayer(in_channels, hidden_channels, heads=heads))
        for _ in range(num_layers - 2):
            self.layers.append(GATLayer(hidden_channels * heads, hidden_channels, heads=heads))
        self.layers.append(GATLayer(hidden_channels * heads, out_channels, heads=1))
        self.global_pool = global_mean_pool

    def forward(self, x, edge_index, batch):
        for layer in self.layers:
            x = layer(x, edge_index)
        graph_feat = self.global_pool(x, batch)
        return graph_feat

# Define MLPModel class
class MLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(MLPModel, self).__init__()
        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(input_dim, hidden_dim))
        for _ in range(num_layers - 1):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
        self.layers.append(nn.Linear(hidden_dim, output_dim))

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = F.relu(self.layers[i](x))
        x = self.layers[-1](x)
        return x

# Use Huber loss
class HuberLoss(nn.Module):
    def __init__(self, delta=1.0):
        super(HuberLoss, self).__init__()
        self.delta = delta

    def forward(self, output, target):
        error = output - target
        is_small_error = torch.abs(error) <= self.delta
        squared_loss = 0.5 * error ** 2
        linear_loss = self.delta * (torch.abs(error) - 0.5 * self.delta)
        return torch.where(is_small_error, squared_loss, linear_loss).mean()

# Training and evaluation function
def train_and_evaluate(mpnn_model, mlp_model, features_smiles1, features_smiles2, numeric_features, target, device, epochs=1):
    X_train, X_test, smiles_train1, smiles_test1, smiles_train2, smiles_test2, y_train, y_test = train_test_split(
        numeric_features, features_smiles1, features_smiles2, target, test_size=0.2, random_state=42)

    y_scaled_train = np.log(y_train + 1)
    y_scaled_test = np.log(y_test + 1)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    y_train_tensor = torch.tensor(y_scaled_train, dtype=torch.float32, device=device).unsqueeze(1)
    y_test_tensor = torch.tensor(y_scaled_test, dtype=torch.float32, device=device).unsqueeze(1)

    criterion = HuberLoss(delta=1.0)
    optimizer = torch.optim.AdamW(mlp_model.parameters(), lr=1e-4, weight_decay=1e-3)

    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32, device=device), y_train_tensor)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4090, shuffle=True, num_workers=0)

    best_r2 = float('-inf')

    for epoch in range(epochs):
        mlp_model.train()
        epoch_losses = []

        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()

            batch_smiles1 = smiles_train1[:batch_X.size(0)].to(device)
            batch_smiles2 = smiles_train2[:batch_X.size(0)].to(device)

            combined_features = torch.cat((batch_smiles1, batch_smiles2, batch_X), dim=1)
            output = mlp_model(combined_features)

            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()
            epoch_losses.append(loss.item())

        if epoch % 100 == 0 or epoch == epochs - 1:
            mlp_model.eval()
            with torch.no_grad():
                smiles_test1_tensor = smiles_test1.to(device)
                smiles_test2_tensor = smiles_test2.to(device)
                X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
                combined_test_features = torch.cat((smiles_test1_tensor, smiles_test2_tensor, X_test_tensor), dim=1)

                pred = mlp_model(combined_test_features)
                y_test_np = y_test_tensor.cpu().numpy()
                pred_np = pred.cpu().numpy()

                test_loss = mean_squared_error(y_test_np, pred_np)
                r2 = r2_score(y_test_np, pred_np)

                abs_errors = np.abs(np.exp(pred_np) - np.exp(y_test_np))
                aard = np.mean(abs_errors / np.exp(y_test_np))

                print(f'Epoch {epoch} | Train Loss: {np.mean(epoch_losses):.4f} | Test Loss: {test_loss:.4f} | R²: {r2:.4f} | AARD: {aard:.4f}')

                if r2 > best_r2:
                    best_r2 = r2

    print(f'Best R² Score: {best_r2:.4f}')
    return best_r2

# Extract feature vectors
def extract_features_and_cache(model, smiles_list, device):
    model.eval()
    features = []
    valid_smiles = []
    invalid_indices = []
    valid_indices = []

    for idx, smiles in enumerate(smiles_list):
        try:
            data = smiles_to_graph(smiles)
            data.batch = torch.zeros(data.x.size(0), dtype=torch.long)
            data = data.to(device)
            with torch.no_grad():
                feat = model(data.x, data.edge_index, data.batch)
                feat = feat.squeeze(0)
                features.append(feat)
                valid_smiles.append(smiles)
                valid_indices.append(idx)
        except ValueError as e:
            print(f"Row {idx} contains an invalid SMILES: {smiles}")
            invalid_indices.append(idx)

    if len(features) == 0:
        raise ValueError("No valid SMILES strings found!")

    return torch.stack(features), valid_smiles, invalid_indices, valid_indices

# Define function to calculate model predictions
def model_predict(smiles1, smiles2, numeric_features, mpnn_model, mlp_model, device):
    with torch.no_grad():
        features_smiles1, _, _, _ = extract_features_and_cache(mpnn_model, smiles1, device)
        features_smiles2, _, _, _ = extract_features_and_cache(mpnn_model, smiles2, device)

        if numeric_features.size(0) == 1:
            smiles1_repeated = features_smiles1[0].unsqueeze(0)
            smiles2_repeated = features_smiles2[0].unsqueeze(0)
        else:
            repeat_factor = numeric_features.size(0) // features_smiles1.size(0)
            smiles1_repeated = features_smiles1.repeat(repeat_factor, 1)
            smiles2_repeated = features_smiles2.repeat(repeat_factor, 1)

        combined_features = torch.cat((smiles1_repeated, smiles2_repeated, numeric_features), dim=1)
        output = mlp_model(combined_features)
    return output.cpu().numpy()

# Perform SHAP interpretation
def compute_shap_values(smiles1, smiles2, numeric_features, mpnn_model, mlp_model, device, k=50):
    numeric_features_np = numeric_features.cpu().numpy()

    background = shap.kmeans(numeric_features_np, k)

    smiles1_background = smiles1[:k]
    smiles2_background = smiles2[:k]

    explainer = shap.KernelExplainer(
        lambda X: model_predict(smiles1_background, smiles2_background,
                                torch.tensor(X, dtype=torch.float32, device=device), mpnn_model, mlp_model, device),
        background.data
    )

    shap_values = explainer.shap_values(numeric_features_np, nsamples=100)
    return shap_values

if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    file_path = r'...'
    df = pd.read_csv(file_path)

    exclude_cols = ['Component#1', 'Component#2', 'Reference (DOI)', 'Viscosity, cP']
    feature_cols = [col for col in df.columns if col not in exclude_cols]

    target_column = 'Viscosity, cP'
    target = df[target_column].values
    numeric_df = df[feature_cols].select_dtypes(include=[np.number])
    numeric_df = numeric_df.fillna(numeric_df.mean())
    numeric_features = numeric_df.values
    print(f"Original feature_cols count: {len(feature_cols)}")

    numeric_feature_cols = numeric_df.columns.tolist()
    print(f"Numeric feature columns count: {len(numeric_feature_cols)}")

    numeric_features_df = pd.DataFrame(numeric_features, columns=numeric_feature_cols)
    numeric_features_df.to_csv("numeric_features.csv", index=False)
    print("numeric_features saved as 1.0_numeric_features.csv")

    smiles_list1 = df['Component#1_SMILES'].tolist()
    smiles_list2 = df['Component#2_SMILES'].tolist()

    mpnn_model = MPNNModel(in_channels=8, hidden_channels=64, out_channels=32, num_layers=6).to(device)

    print("Extracting and caching features for Component#1...")
    features_smiles1, valid_smiles1, invalid_indices1, valid_indices1 = extract_features_and_cache(mpnn_model, smiles_list1, device)

    print("Extracting and caching features for Component#2...")
    features_smiles2, valid_smiles2, invalid_indices2, valid_indices2 = extract_features_and_cache(mpnn_model, smiles_list2, device)

    valid_indices = list(set(valid_indices1).intersection(valid_indices2))
    numeric_features = numeric_features[valid_indices]
    target = target[valid_indices]

    mlp_model = MLPModel(input_dim=features_smiles1.shape[1] + features_smiles2.shape[1] + numeric_features.shape[1],
                         hidden_dim=128, output_dim=1, num_layers=3).to(device)

    avg_r2 = train_and_evaluate(mpnn_model, mlp_model, features_smiles1, features_smiles2, numeric_features, target, device)

    numeric_features_tensor = torch.tensor(numeric_features, dtype=torch.float32, device=device)

    shap_values = compute_shap_values(valid_smiles1, valid_smiles2, numeric_features_tensor, mpnn_model, mlp_model,
                                      device)

    shap_values_np = np.array(shap_values)
    print(shap_values_np)

    shap_values_2d = shap_values_np[0]

    shap_df = pd.DataFrame(shap_values_2d)

    shap_df.to_csv("...", index=False)
