from torch_geometric.utils import from_networkx
from torch_geometric.nn import GATConv, global_mean_pool
from rdkit import Chem
import networkx as nx
import torch
import torch.nn.functional as F
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

def atom_features(atom):
    return [
        atom.GetAtomicNum(),
        atom.GetDegree(),
        atom.GetHybridization(),
        atom.GetIsAromatic(),
        atom.GetTotalNumHs(),
        atom.GetExplicitValence(),
        atom.GetNumImplicitHs(),
        atom.GetFormalCharge()
    ]

def smiles_to_graph(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        raise ValueError(f"Invalid SMILES: {smiles}")
    G = nx.Graph()
    for atom in mol.GetAtoms():
        G.add_node(atom.GetIdx(), x=torch.tensor(atom_features(atom), dtype=torch.float))
    for bond in mol.GetBonds():
        G.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())
    data = from_networkx(G)
    return data

class GATLayer(nn.Module):
    def __init__(self, in_channels, out_channels, heads=8):
        super(GATLayer, self).__init__()
        self.conv = GATConv(in_channels, out_channels, heads=heads)

    def forward(self, x, edge_index):
        return self.conv(x, edge_index)

class MPNNModel(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, heads=4):
        super(MPNNModel, self).__init__()
        self.layers = torch.nn.ModuleList()
        self.layers.append(GATLayer(in_channels, hidden_channels, heads=heads))
        for _ in range(num_layers - 2):
            self.layers.append(GATLayer(hidden_channels * heads, hidden_channels, heads=heads))
        self.layers.append(GATLayer(hidden_channels * heads, out_channels, heads=1))
        self.global_pool = global_mean_pool

    def forward(self, x, edge_index, batch):
        for layer in self.layers:
            x = layer(x, edge_index)
        graph_feat = self.global_pool(x, batch)
        return graph_feat

class MLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(MLPModel, self).__init__()
        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(input_dim, hidden_dim))
        for _ in range(num_layers - 1):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
        self.layers.append(nn.Linear(hidden_dim, output_dim))

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = F.relu(self.layers[i](x))
        x = self.layers[-1](x)
        return x

class HuberLoss(nn.Module):
    def __init__(self, delta=1.0):
        super(HuberLoss, self).__init__()
        self.delta = delta

    def forward(self, output, target):
        error = output - target
        is_small_error = torch.abs(error) <= self.delta
        squared_loss = 0.5 * error ** 2
        linear_loss = self.delta * (torch.abs(error) - 0.5 * self.delta)
        return torch.where(is_small_error, squared_loss, linear_loss).mean()

def ensemble_predictions(models, features):
    predictions = np.zeros((features.shape[0], len(models)))
    for i, model in enumerate(models):
        predictions[:, i] = model.predict(features)
    return np.mean(predictions, axis=1)

def train_and_evaluate(mpnn_model, mlp_model, features_smiles1, features_smiles2, numeric_features, target, device, epochs=2000):
    y_scaled = np.log(target + 1)
    scaler = StandardScaler()
    numeric_features_scaled = scaler.fit_transform(numeric_features)
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    best_r2_scores = []

    for train_index, test_index in kf.split(numeric_features_scaled):
        X_train, X_test = numeric_features_scaled[train_index], numeric_features_scaled[test_index]
        smiles_train1, smiles_test1 = features_smiles1[train_index], features_smiles1[test_index]
        smiles_train2, smiles_test2 = features_smiles2[train_index], features_smiles2[test_index]
        y_train, y_test = y_scaled[train_index], y_scaled[test_index]

        y_train = torch.tensor(y_train, dtype=torch.float32, device=device).unsqueeze(1)
        y_test = torch.tensor(y_test, dtype=torch.float32, device=device).unsqueeze(1)

        criterion = HuberLoss(delta=1.0)
        optimizer = torch.optim.AdamW(mlp_model.parameters(), lr=1e-4, weight_decay=1e-3)

        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32, device=device), y_train)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4090, shuffle=True, num_workers=0)

        best_r2 = float('-inf')
        for epoch in range(epochs):
            mlp_model.train()
            epoch_losses = []

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                optimizer.zero_grad()

                batch_smiles1 = smiles_train1[:batch_X.size(0)].to(device)
                batch_smiles2 = smiles_train2[:batch_X.size(0)].to(device)

                combined_features = torch.cat((batch_smiles1, batch_smiles2, batch_X), dim=1)
                output = mlp_model(combined_features)

                loss = criterion(output, batch_y)
                loss.backward()
                optimizer.step()
                epoch_losses.append(loss.item())

            if epoch % 100 == 0 or epoch == epochs - 1:
                mlp_model.eval()
                with torch.no_grad():
                    combined_test_features = torch.cat((smiles_test1, smiles_test2), dim=1)
                    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)
                    combined_test_features = torch.cat((combined_test_features, X_test_tensor), dim=1)

                    pred = mlp_model(combined_test_features)

                    y_test_np = y_test.cpu().numpy()
                    pred_np = pred.cpu().numpy()

                    test_loss = mean_squared_error(y_test_np, pred_np)
                    r2 = r2_score(y_test_np, pred_np)

                    abs_errors = np.abs(np.exp(pred_np) - np.exp(y_test_np))
                    aard = np.mean(abs_errors / np.exp(y_test_np))

                    print(f'Epoch {epoch} | Train Loss: {np.mean(epoch_losses):.4f} | Test Loss: {test_loss:.4f} | R²: {r2:.4f} | AARD: {aard:.4f}')

                    if r2 > best_r2:
                        best_r2 = r2

        best_r2_scores.append(best_r2)

    avg_best_r2 = np.mean(best_r2_scores)
    print(f'Cross-Validation Best R² Score: {best_r2_scores:.4f}')
    print(f'Cross-Validation Mean Best R² Score: {avg_best_r2:.4f}')
    return best_r2, best_r2_scores

def extract_features_and_cache(model, smiles_list, device):
    model.eval()
    features = []
    valid_smiles = []
    invalid_indices = []
    valid_indices = []

    for idx, smiles in enumerate(smiles_list):
        try:
            data = smiles_to_graph(smiles)
            data.batch = torch.zeros(data.x.size(0), dtype=torch.long)
            data = data.to(device)
            with torch.no_grad():
                feat = model(data.x, data.edge_index, data.batch)
                feat = feat.squeeze(0)
                features.append(feat)
                valid_smiles.append(smiles)
                valid_indices.append(idx)
        except ValueError as e:
            print(f"Row {idx} contains an invalid SMILES: {smiles}")
            invalid_indices.append(idx)

    if len(features) == 0:
        raise ValueError("No valid SMILES strings found!")

    return torch.stack(features), valid_smiles, invalid_indices, valid_indices

if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    file_path = r' '
    df = pd.read_csv(file_path)

    exclude_cols = ['Component#1', 'Component#2', 'Reference (DOI)', 'Viscosity, cP']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    target_column = 'Viscosity, cP'
    target = df[target_column].values
    numeric_df = df[feature_cols].select_dtypes(include=[np.number])
    numeric_df = numeric_df.fillna(numeric_df.mean())
    numeric_features = numeric_df.values
    
    smiles_list1 = df['Component#1_SMILES'].tolist()
    smiles_list2 = df['Component#2_SMILES'].tolist()
    
    mpnn_model = MPNNModel(in_channels=8, hidden_channels=64, out_channels=32, num_layers=6).to(device)
    
    print("Extracting and caching features for Component#1...")
    features_smiles1, valid_smiles1, invalid_indices1, valid_indices1 = extract_features_and_cache(mpnn_model, smiles_list1, device)
    
    print("Extracting and caching features for Component#2...")
    features_smiles2, valid_smiles2, invalid_indices2, valid_indices2 = extract_features_and_cache(mpnn_model, smiles_list2, device)
    
    valid_indices = list(set(valid_indices1).intersection(valid_indices2))
    numeric_features = numeric_features[valid_indices]
    target = target[valid_indices]
    
    mlp_model = MLPModel(input_dim=features_smiles1.shape[1] + features_smiles2.shape[1] + numeric_features.shape[1],
                         hidden_dim=128, output_dim=1, num_layers=3).to(device)
    
    avg_r2, r2_scores = train_and_evaluate(mpnn_model, mlp_model, features_smiles1, features_smiles2, numeric_features, target, device)
